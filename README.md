
# ğŸ“š Tech Challenge - API PÃºblica de Livros

Projeto desenvolvido para o Tech Challenge da PÃ³s-Tech, Fase 1, com foco em Engenharia de Machine Learning.

O desafio propÃµe a criaÃ§Ã£o de um pipeline de dados com web scraping, transformaÃ§Ã£o e disponibilizaÃ§Ã£o dos dados de livros via API RESTful, com foco em **escalabilidade** e **reusabilidade** para futuros modelos de machine learning.

---

## ğŸ¯ Objetivo

- Criar um pipeline de dados completo.
- Disponibilizar os dados via API pÃºblica.
- Preparar a soluÃ§Ã£o para futuros usos em projetos de ML.
- Publicar a soluÃ§Ã£o em ambiente de produÃ§Ã£o com esteira CI/CD.

## ğŸ§© Arquitetura da SoluÃ§Ã£o

![Arquitetura da SoluÃ§Ã£o](Insumos/Arquitetura%20da%20soluÃ§Ã£o.jpg)

## ğŸš€ Processo de Deploy

![Fluxo de Deploy](Insumos/fluxo_de_deploy.jpg)


---

## ğŸ“Œ Premissas Atendidas

- Pipeline completo de dados
- API REST funcional
- Web scraping robusto
- Dados estruturados para ML
- Deploy pÃºblico disponÃ­vel

---

## ğŸ—‚ Estrutura do RepositÃ³rio

```
MLET_TC01/
â”œâ”€â”€ api/                  # ImplementaÃ§Ã£o da API (FastAPI)
â”œâ”€â”€ scripts/              # Script de Web Scraping (scraper.py)
â”œâ”€â”€ models/               # Modelos ORM com SQLAlchemy
â”œâ”€â”€ database/             # ConexÃ£o e setup do SQLite
â”œâ”€â”€ data/                 # Armazenamento local dos dados (.csv)
â”œâ”€â”€ ingest_data.py        # Script de ingestÃ£o de dados no banco
â”œâ”€â”€ requirements.txt      # DependÃªncias do projeto
â””â”€â”€ README.md             # Este arquivo
```


---

## ğŸ”— Endpoints da API

### Core
- `GET /api/v1/books` â€“ Lista todos os livros
- `GET /api/v1/books/{id}` â€“ Detalhes de um livro
- `GET /api/v1/books/search?title=&category=` â€“ Busca por tÃ­tulo e/ou categoria
- `GET /api/v1/categories` â€“ Lista de categorias
- `GET /api/v1/health` â€“ Verifica status da API

## ğŸ” AutenticaÃ§Ã£o com JWT

O projeto conta com autenticaÃ§Ã£o implementada usando **JWT (JSON Web Tokens)**.

### Rotas de autenticaÃ§Ã£o:

- `POST /api/v1/auth/login` â€“ Realiza o login e retorna um token JWT.
- `POST /api/v1/auth/refresh` â€“ Gera um novo token com base no token de refresh.

### Endpoints protegidos

- Endpoints sensÃ­veis como `/api/v1/scraping/trigger` estÃ£o protegidos e exigem um token vÃ¡lido.
- Para acessar esses endpoints, inclua o header:
```
Authorization: Bearer <seu_token>
```

Essa implementaÃ§Ã£o garante seguranÃ§a bÃ¡sica para administraÃ§Ã£o da API e controle de acesso Ã s operaÃ§Ãµes crÃ­ticas.

### Insights (opcional)
- `GET /api/v1/stats/overview` â€“ EstatÃ­sticas gerais
- `GET /api/v1/stats/categories` â€“ EstatÃ­sticas por categoria
- `GET /api/v1/books/top-rated` â€“ Melhores avaliaÃ§Ãµes
- `GET /api/v1/books/price-range?min=&max=` â€“ Faixa de preÃ§o

---

## ğŸ§© AWS

1. **IngestÃ£o:**

A ingestÃ£o Ã© realizada por uma lambda function que pode ser disparada manualmente ou via Schedule atraves do AWS Event Bridge(crontab)
A Lambda function realiza o scraping da informaÃ§Ãµes de livros do site https://books.toscrape.com/

![Lambda_1](Insumos/Lambda_screen1.png)
![Lambda_2](Insumos/Lambda_screen2.png)

2. **Armazenamento:**

O resultado da funÃ§Ã£o Lambda Ã© um arquivo csv no S3  

![s3csv](Insumos/S3CSV.png)


## ğŸ§© SNOWFLAKE

1. **IngestÃ£o:**





---

## âš™ï¸ Como Executar Localmente

1. **Clone o projeto:**
```bash
git clone https://github.com/seuusuario/MLET_TC01.git
cd MLET_TC01
```

2. **Crie e ative o ambiente virtual:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

4. **Execute o scraping:**
```bash
python scripts/scraper.py
```

5. **IngestÃ£o dos dados no banco:**
```bash
python ingest_data.py
```

6. **Execute a API localmente:**
```bash
uvicorn api.main:app --reload
```


---

## ğŸ§© Arquitetura da SoluÃ§Ã£o - ACHO QUE PODEMOS REMOVER ESSA PARTE - DUPLICIDADE

### ğŸ”§ VisÃ£o Geral

```
+-------------+      +-------------+      +--------------------+      +-------------+
|   Fonte     | ---> |  IngestÃ£o   | ---> |  Armazenamento     | ---> |   Consumo   |
| (HTML Site) |      |  (Lambda)   |      | (CSV + SQLite DB)  |      | (FastAPI)   |
+-------------+      +-------------+      +--------------------+      +-------------+
```

### ğŸ” Etapas do Processo 

1. **Scraping** dos dados do site [books.toscrape.com](https://books.toscrape.com/)
2. Armazenamento dos dados em `.csv`
3. IngestÃ£o no banco SQLite
4. TransformaÃ§Ã£o de campos
5. ExposiÃ§Ã£o via **API RESTful**
6. Consumo por cientistas de dados ou usuÃ¡rios

---

## ğŸš€ Processo de Deploy

O projeto conta com esteira de CI/CD estruturada para deploy automÃ¡tico:

### ğŸ”„ Fluxo

1. Push do cÃ³digo na branch `main`
2. GitHub Actions executa CI com testes e validaÃ§Ãµes
3. Deploy automÃ¡tico em ambiente de **staging (AWS Lambda)**
4. Deploy final em **produÃ§Ã£o (Render)**



---

## ğŸ¥ ApresentaÃ§Ã£o

[ğŸ”— Link para vÃ­deo de apresentaÃ§Ã£o (inserir link quando disponÃ­vel)]

---

## ğŸ’¡ PrÃ³ximos Passos (Extras)

- ğŸ”’ Implementar autenticaÃ§Ã£o com JWT
- ğŸ“Š Criar dashboard com Streamlit
- ğŸ§  Adicionar endpoints para ML
